\section{Issues}
\label{sec:issues}

% todo: write intro

\subsection{Benchmarking system}
\label{subsec:benchmarking_system}
Larry Page (Google founder) advises people to ``fail fast''.
By this he means that one has to try things and if some plan does not work one has to quickly realise that and move on.
This appears to be sound advise for business and research alike.
With that in mind it is time to concede defeat for the benchmarking system.
It is found to be highly impractical.

% data, dependencies, payments, depending on use case, also company preferences matter
Datasets, for example, are often not publicly available.
This is probably caused by the sensitive nature of natural language.
Dependencies increase the maintenance costs.
The dependencies are in the form of application programming interfaces (APIs).
APIs are used by software and will therefore not change often.
Eventually they will do, ergo eventually the benchmarking software needs to be updated.
Another problem is that the services which offer APIs are not open.
When the benchmark is extended to include all systems then all API keys need to be added as well.
The owner of the benchmarking tool can decide to offer paid keys or let users set keys manually.
The latter requires users to have an account for each service.
A closed-source solution is Intento (\url{https://inten.to}).
One can send data to the site via their API and they will run a benchmark on various services for a given task.
Their `catalog' contains machine translation, intent detection, sentiment analysis, text classification, dictionaries, image tagging, optical character recognition and speech-to-text.
The final issue with \textsc{bench} is as follows.
When choosing a system not only the performance matters.
One reason for this is that accuracies are volatile.
If companies would base their decision solely on `the highest accuracy' then they would need to change system each month.
Since companies cannot spend their time constantly switching systems they should spend their time on other factors.
These factors can include pricing, privacy (whether open-source) and in-house API preferences.

\subsection{Methodology}
\label{subsec:methodology}
Creating a benchmarking tool has resulted in more insight into intent classification.
This helped in identifying issues in the methodology proposed by~\citet{braun2017}.
The created datasets and many presented ideas are useful, however some improvements are possible.
As discussed in section~\ref{subsec:available_datasets} falling back to `None' or a random intent changes f1 score.
In the paper Chatbot does not have a `None' intent, while WebApplications and AskUbuntu do.
Furthermore, drawn conclusions about some system being more accurate than others seems insubstantial.
The conclusion that accuracy of some system depends on the domain seems convincing, but is poorly grounded.
Reason for this is that both conclusions are based on the f1 score.

% average
In this paper the f1 score is calculated using micro f1 score.
Such a score does not take classes of different size, so called class imbalances, into account.
This is combined with a situation where intents and entities are given the same weight.
For WebApplications there are in total 74 labeled intents and 151 labeled entities.
AskUbuntu contains 128 labeled intents and 123 labeled entities.
So, when using micro f1 on AskUbuntu the score is based somewhat equally on intents and entities.
For WebApplications the score is based for about one thirds on intents and two thirds on entities.
This could mean that some system has scored significantly better that others simply because it labels entities in WebApplications particularly well.
Another reason which makes this score odd is that users interested in either intent or entity classification are not well informed.
Better seems to be using weighted f1.
Here the f1 score for each class is multiplied (weighted) by the number of elements in that particular class.
Imbalances can also be handled by calculating a macro f1 score, but this is more computational expensive.
Rasa, for example, uses the weighted average in their evaluation according to (\url{https://github.com/RasaHQ/rasa_nlu/blob/master/rasa_nlu/evaluate.py}).

% non-deterministic probalistic models, showing detailed information is misleading
Another reason to distrust presented f1 scores is the probabilistic nature of neural networks.
Although inference (classification) is deterministic, training is not.
During training models often start with random weights.
Random initializations can move into different local minima for the same training data.
This could change the inference results.
During benchmarking this effect has been observed for Rasa using the Spacy back-end.
According to a mail from the main author Rasa 0.5 with the MITIE back-end is used for the results described in the paper.
The MITIE back-end has not shown to change accuracy after re-training the model.

\subsection{State of field}

