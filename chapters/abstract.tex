\chapter*{Abstract}
\label{ch:abstract}

In the last few years artificial intelligence has considerably advanced the natural language processing (NLP) field.
It is interesting to see whether this can be used to automate customer support.
NLP has evolved to contain many tasks.
The intent classification combined with named-entity recognition (NER) are the most interesting due to the following practical constraints.
Intents and entities are often used by conversational agents (advanced chatbots) and should therefore classify in real-time.
Also, the training data typically contains only a few dozen examples.

Accuracy of classification differs per system.
Higher accuracy means responding correctly to customer utterances more often.
Many systems claim to have the best accuracy scores when comparing their system to others.
This is highly suspicious.
To solve this a benchmarking tool is created.
The tool is aimed on creating comparisons in such a way that users can easily run new or re-run existing evaluations.
The code can be extended to allow comparison of more datasets and systems.

To improve the accuracy of intent and named-entity classification systems deep learning architectures for NLP have been investigated.
New accuracy records are set every few months for various NLP tasks.
One of the most promising systems at the time of writing is considered.
The system, Google BERT, uses context from both sides of some word to predict the meaning of the word.
BERT has shown state-of-the-art results for eleven NLP tasks.
An attempt is made to increase that number by applying the model to intent classification.
This obtained significant increases in running time, but not in accuracy.
A second attempt trained the system jointly on intent classification and named-entity recognition.
BERT is well-suited for joint training, because it uses context in all hidden layers of the network.
Information from the intent classification task is used for making NER predictions and vice versa.
It is shown that joint training with BERT is feasible, and can be used to lower training time when compared to separate training of BERT.
Future work is needed to see whether accuracy improvements are significant.
