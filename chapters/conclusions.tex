\chapter{Conclusions}
\label{ch:conclusions}
% intro
Recent well-known artificial intelligence (AI) examples, such as Apple Siri, suggest that difficult natural language processing (NLP) tasks can be automated.
The graduation company is interested to see whether this can be used to automate customer support.
To this end various NLP tasks have been considered.
Eventually it is decided that intent classification and named-entity recognition are an interesting combination of tasks for the graduation company.
This combination of tasks is called natural language understanding (NLU) and often used in chatbots to respond to users in real-time.
% research question one
While reading about this task it was found that various parties run benchmarks and draw conclusions.
For each party an issue which affects the validity is found.
This gives rise to the following research question and goal.\\

\rqone\\

\rgone\\

The answer for the first research question is that it is possible.
However, stronger requirements are needed to make the tool more useful.
The tool is required to be continuously maintained to adapt to changing APIs for NLU services and software.
It would require to offer an API key, or let users add their own keys.
Furthermore it should offer better metrics.
Deciding on a product depends not only on accuracy, but also depends on at least pricing, memory usage and running time.
The system should include more metrics to reflect these properties.
Currently the accuracy metric is based on the last run.
Training is probabilistic, meaning that accuracy may vary for each training run.
Reporting only the last run can lead to incorrect conclusions, so the program should execute multiple runs and summarize the results.
Lastly, the used datasets are small or domain specific.
More and bigger datasets would allow for more statistical powerful conclusions.

%research question two
Next, the tool (and knowledge obtained by creating the tool) is used to work on the following research question and goal.\\

\rqtwo\\

\rgtwo\\

The search for improvement has considered increasing the amount of training data and using new meta-learning algorithms and embeddings.
A recent model called Google BERT~\citep{devlin2018} is expected to be the most likely candidate for increasing accuracy.
The model provides a pre-trained checkpoint which has `learned' about language by reading large amounts of text.
The pre-trained checkpoint can then be fine-tuned on some specific NLP task using transfer learning.
It is a big model, meaning that fine-tuning takes around 1,5 days on a modern computer and a few minutes on a high-end GPU.
Experiments on intent classification datasets show non-significant improvements in accuracy.
To improve accuracy further the model has been jointly trained on intent classification and named-entity recognition.
The benefit is that named-entity information can be used to determine the intent and vice versa.
The Google model is a good candidate for jointly training because it, unlike other recent models, uses left and right context in all layers of the network.
BERT has obtained state-of-the-art results in a wide range of tasks including named-entity recognition.
These two facts imply that jointly training BERT should obtain state-of-the-art results on the joint intent classification and NER task.
Basic experiments are conducted in which training BERT separately is compared to training it jointly.
The experiments show that jointly training is possible and compared to separate training obtains higher accuracies for intent classification and NER while requiring fewer training steps.
Compared to a baseline the intent accuracy is equal or higher for datasets with around 100 training examples.
Future work is needed to see whether the model implementation can be improved and whether the improvements in accuracy are significant.

\iffalse
% conclusions
In general the NLP field is in an interesting state.
Technology companies have a lot of incentive to push the field forward.
Leaps in the last few years have come from those companies.
For example, FastText by Facebook and transformer models by Google.
A second observation is that state-of-the-art scores are increased every few months.
This results in papers which are quickly pushed to arXiv and cited before any scholarly peer review.
Papers report accuracy high scores with ``few mentions of average cases and variability or worst-cases''~\citep{otter2018survey}.

A recent review paper for deep learning and NLP~\citep{young2018recent} still see lots of potential.
They expect better models for unlabelled data, reinforcement learning, zero-shot learning and network memory enrichment via a knowledge base.

\fi
% this cautionary conclusion is mainly from first part of thesis
% check number of arxiv citations in review paper, ~64 of 190 citations are arXiv

% todo: mention that the approach generalizes to task
