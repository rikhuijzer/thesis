\chapter{Conclusions}
\label{ch:conclusions}
% intro
Media suggests that difficult natural language processing (NLP) tasks can be solved by using artificial intelligence.
It is interesting to see whether this can be used to automate customer support.
To this end various NLP tasks have been considered.
Eventually it is decided that intent classification is the most interesting task.
Intent classification attempts to classify the intention of some user when he utters some sentence.
For example, ``what is the weather tomorrow?'' could be labeled as `get\_weather'.
A conversational agent (advanced chatbot) could then use this intent to decide how to respond.
% research question one
While reading about this task it was found that various systems claim to have obtained the highest accuracy scores for certain datasets.
This is highly suspicious and gives rise to the following research question and goal.\\

\rqone\\

\rgone\\

The answer for the first research question is that it is possible, but impractical.
Main issue is that to test a service one has to make API calls.
This requires the user of a benchmark tool to have an user account for each service and it forces the tool to update for each changing service.
Another issue is that evaluation of results is done on pre-defined datasets.
This does not guarantee that some system is indeed the best choice for some problem at hand.
It could be that the pre-defined dataset contains more training data, is in another domain or uses another language.
Even when using the benchmarking tool with some use-case specific dataset knowing the best system is not very useful.
Services push new models into production without letting users know, so benchmark results can become invalid at any moment.

%research question two
Next, the tool (and knowledge obtained by creating the tool) is used to work on the following research question and goal.\\

\rqtwo\\

\rgtwo\\

The search for improvement has considered increasing the amount of training data and using new meta-learning algorithms and embeddings.
A recent model called Google BERT~\citep{devlin2018} is expected to be the most likely candidate for increasing accuracy.
The model uses pre-training to learn about language and can be fine-tuned to some specific task.
It is a big model, meaning that fine-tuning takes around 1,5 days on a modern computer and a few minutes on a high-end GPU.
Experiments on intent classification datasets showed non-significant improvements in accuracy.
To improve accuracy further the model has been jointly trained on intent classification and named-entity recognition.
The benefit is that named-entity information can be used to determine the intent and vice versa.
The Google model is a good candidate for jointly training because it uses left and right context in all layers (deep bidirectionality).
BERT has obtained state-of-the-art results in a wide range of tasks including named-entity recognition.
This implies that jointly training BERT should obtain state-of-the-art results on the natural language understanding task.
Specifically, on tasks where data consists of intents and named-entities which is typical for conversational agents.
Basic experiments are conducted in which training BERT separately is compared to training it jointly.
The experiments show that jointly training is possible and in some cases obtains higher accuracies than separate training.
It is expected that the results can be improved by taking a second look at the implementation and improving it.
One issue for the current implementation is that the implementation does not shuffle the training data.

\iffalse
% conclusions
In general the NLP field is in an interesting state.
Technology companies have a lot of incentive to push the field forward.
Leaps in the last few years have come from those companies.
For example, FastText by Facebook and transformer models by Google.
A second observation is that state-of-the-art scores are increased every few months.
This results in papers which are quickly pushed to arXiv and cited before any scholarly peer review.
Papers report accuracy high scores with ``few mentions of average cases and variability or worst-cases''~\citep{otter2018survey}.
\fi
% this cautionary conclusion is mainly from first part of thesis
% check number of arxiv citations in review paper, ~64 of 190 citations are arXiv

% todo: mention that the approach generalizes to task
