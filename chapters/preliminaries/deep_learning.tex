\section{Deep learning}
\label{sec:deep_learning}

As with many computer science subfields deep learning has outperformed manual feature engineering on many NLP tasks.
For the last few years the best performing NLP systems have been neural networks~\citep{young2018recent}.
This section will provide a basic overview of the most important model architectures for NLP.
% TODO: Explain encoder decoder.
% TODO: Explain transfer learning? (used for ELMo)

\iffalse
\subsection{word2vec}
\label{subsec:word2vec}
This section will take one step in the direction of using deep learning for NLP.
Neural networks are basically a mapping from vector to vector (numbers to numbers).
Natural language can be converted from and to vectors using embeddings.
This section will introduce embeddings and explain some common concepts.
The focus lies on the intuition behind the approaches.
The goal of this is being able to estimate suitability of certain approaches for certain tasks.


\subsection{GloVe}
\label{subsec:glove}
\fi

\subsection{Vanishing gradient problem}
\label{subsec:vanishing_gradient_problem}
The vanishing gradient problem, as recognized by~\citet{hochreiter2001gradient}, is one of the main issues for training neural networks.
The backpropagation algorithm updates the weights in the network by sending information from the output layer in the direction of the input layer.
The problem relates to vanishing and exploding updates to weights in the layers further from the output layer.
These extreme updates are caused by the backpropagation algorithm.
Consider some weight $w$ which is near the input layer of some network.
This weight is updated by the following partial derivative $w = d_1 \cdot d_2 \cdot \ldots \cdot d_i$.
Here $i$ corresponds to the number of layers in the network.
These partial derivatives $d_x$ can become small $(0 \leq d_x < 1)$ or large $(1 \ll d_x)$.
Typically the learning rate has an order of magnitude of \numb{1}{-3}.
When one derivative is small the weight update when multiplied by learning rate might become very small.
Conversely when one derivative is big the update might become very big.
Since the weight is near the input layer, $i$ is big.
Thus, the chance that weights further away from the output layer vanish or explode increase by increment of $i$.
Exploding gradients can be solved by putting a threshold on the update~\citep{mikolov2013distributed}.
Vanishing gradients are more difficult since it is unknown whether an update is small due to vanishing gradients or due to the fact that the weight should not be changed according to the loss function.
Effectively the vanishing gradient problem causes layers which are have a long distance from the output layer to stop learning.

\subsection{Recurrent neural networks}
\label{subsec:rnn}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{figures/rnn.jpg}
    \end{center}
    \caption{Recurrent neural network unfolded in time~\cite[Figure 5]{lecun2015deep}.}
    \label{fig:rnn}
\end{figure}
A recurrent neural network (RNN) is an extension on neural networks which allows the network to have memory.
This is effective for problems where the data is sequential.
In a RNN the information from the state of the network is passed to the next state, see Figure~\ref{fig:rnn}.
This state contains information from previous states, we call this a `summary', denoted $W$ in the image.
On the right side the model is unfolded in time.
The unfolded representation shows the states of the network and the flow of information for three consecutive points in time.
To know its history the neural network in current state $s_t$ obtains the summary $W$ from the previous state $s_{t-1}$.
Suppose we are training a RNN in an unsupervised way.
Hence, the model trains on unlabeled real-world texts.
For each step it is asked to predict the output word $o_t$ based only on all $k$ previous words $x_{t-1}, x_{t-2}, \ldots, x_{t-k}$.
The prediction is compared to the correct word, if these are not equal the loss is backpropagated.
The backpropagation is then able to `change history' to improve prediction accuracy.

The benefit of this architecture over n-grams, as presented in Section~\ref{subsec:language_model}, is that the information is compressed inside the neural network.
Also, there is a certain sense of importance since the weights are not uniformly updated for all previous states (words).
Take, for example, `the car, which is green, broke'.
For this sentence the word `broke' can more easily be predicted based on `the car' than on `which is green'.
It is found that RNNs are able to capture this importance~\citep{manning2017lectures}.

In practice RNNs are not using the complete history.
The cause for this are vanishing gradients.
``In practise gradients vanish and simple RNNs become like 7-grams for most words''~\citep{manning2017lectures}.

\subsection{Gated recurrent units and LSTMs}
\label{subsec:gru}
Basic RNNs do not yet provide a way to translate languages.
Machine translation requires to convert some sentence from a source language to a target language.
To this end a RNN encoder-decoder has been proposed by~\citet{cho2014learning}.

\begin{figure}
    \centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/encoder_decoder.png}
        \caption{RNN Encoder-decoder~\cite[Figure 1]{cho2014learning}.}
    \label{fig:encoder_decoder}
    \end{minipage}
    \hspace*{3mm}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/gru.png}
        \caption{GRU hidden activation function~\cite[Figure 2]{cho2014learning}.}
    \label{fig:gru}
    \end{minipage}
\end{figure}

Similar to the basic RNN the decoder at some time has access to only the previous state, see figure~\ref{fig:encoder_decoder}.
The encoder takes the source sentence of variable length $T$ and maps it to a fixed length vector.
The decoder in turn maps the fixed length vector to the target sentence of length $T'$.
To do this the network reads all words $x_1, x_2, \ldots, x_T$ until an end of line token is received.
At that point the entire sentence is captured in the hidden state $C$.
The decoder starts generating words $y_1, y_2, \ldots, y_{T'}$ based on the previous decoder states and $C$.
This is visualised by the arrows.
The authors of the paper recognize that this approach has the same limitations as the basic RNN.
Typically words in translation sentence pairs are aligned, as described in Section~\ref{subsec:mt}.
For example, when generating the first word $y_1$ the algorithm mainly needs info from $x_1$, but has more recently seen the next words in the sequence $x_2, x_3, \cdots, x_{T'}$.
Vanishing gradients will cause the network to forget its far history, so based on the 7-grams claim RNNs are only effective for translating sentences shorter than 7 words.
To solve this the authors~\citep{cho2014learning} have introduced gates to RNNs.
To improve this gated recurrent architectures such as long short-term memory networks (LSTMs)~\citep{hochreiter1997long} and gated recurrent units (GRUs)~\citep{cho2014learning} have been developed.

GRUs have gates which automatically learn to open and close for some hidden state.
This can be visualised by looking at the information which is passed through the states.
The information is captured in a matrix.
In a RNN the information in the entire matrix is updated in each step.
GRUs learn to read and write more selectively, see figure~\ref{fig:gru_subset}.
For some point in time the update consist of reading a specific subset and writing a specific subset of the matrix.
In effect the network learns to look only at the word it needs~\citep{manning2017lectures}.
For example when translating a sentence from German to English it will look at the verb in German to come up with the verb in English.
Writing, in effect, lets the model allocate specific parts in the matrix for specific parts of speech (for example, nouns and verbs).

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{figures/gru_subset.png}
    \end{center}
    \caption{Simplistic visualisation for updating the hidden state in a GRU.}
    \label{fig:gru_subset}
\end{figure}

LSTMs~\citep{hochreiter1997long} are similar to GRUs.
An LSTM does not only contain update and reset gates but also uses a internal memory state.
In practise LSTMs take longer to converge than GRUs~\citep{chung2014empirical}, but remember around 100 words where GRUs remember around 40 words~\citep{manning2017lectures}.

\iffalse
\subsection{Attention}
\label{subsec:attention}
%See also CS224n including 'advanced attention' slides.
One drawback of recurrent neural networks using the encoder-decoder model is that the decoder obtains all information from the final state of the encoder.
A problem with this can be explained by looking at machine translation.
When translating words are aligned as explained in Section~\ref{subsec:mt}
When generating the $n$-th word during decoding information we generally need mainly information from the $n$-th word which occurred during encoding.
It makes more sense to focus the neural network \textit{attention} on the part of the sentence we actually need the most.
To this end the neural net learns to choose what hidden states are important.
%TODO: Really add image here just like slides.
% The important states get a higher weight which yield a context vector.
% TODO: Way too short, need to explain better.
\fi

\subsection{Bidirectional recurrent neural networks}
\label{subsec:bidirectional}
All recurrent architectures described above use only the information on the left of some word to predict it.
Take for example the following sentences containing the polyseme `drinking'.
\begin{center}
    Peter was drinking after a workout.\\[3mm]
    Peter was drinking excessively.
\end{center}
The meaning of the word `drinking' changes after reading the next words in the sentence.
To take this into account bidirectional recurrent neural networks (BRNN) have been developed by~\citet{schuster1997}.
A BRNN contains two separate RNNs as depicted in figure~\ref{fig:bidirectional}.
The paper only considers RNNs, but the method can be applied to gated recurrent models as well~\citep{peters2018}.
One RNN goes through elements of the sequence from left to right and the other in the reverse direction.
Training can be done by showing the network all words except for one.
Both networks learn to predict the next word given a sequence of words.
Calculating the loss is done by taking the average of the predictions of both RNNs.
To reduce required computational power one simplification is used.
Suppose we want to learn from the word at location $k$, $x_k$, and stepped through many states to reach $s_{k-1}$ and $s_{k+1}'$.
Here we let the model make a prediction for $y_k$.
Then we update the weights and, assuming we go forward, now want to learn from $x_{k+1}$.
The RNN in state $s_{k-1}$ takes one step forward, but the RNN in state $s_{k+1}'$ has to restart from the last word in the sequence.
To solve this both RNNs make one prediction for each word and the answers of both for the entire sequence are batched and used to update the weights.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{figures/bidirectional.png}
    \end{center}
    \caption{Bidirectional RNN~\citep{olah2015}.}
    \label{fig:bidirectional}
\end{figure}

\subsection{Convolutional neural networks}
\label{subsec:cnn}
% cnn intro
Convolutional neural networks (CNNs)~\citep{lecun1995convolutional} are an extension to vanilla neural networks which allow the model to exploit spatial locality.
This makes the networks highly successful for image classification.
During image classification CNNs learn a hierarchical representation of the input, according to~\citet{conneau2016very}.
The authors claim that such an hierarchical representation would have benefits for NLP over sequential models.
One of the benefits is that the depth of the tokens, which can be chosen as words but also characters, varies for the tokens in the sentence.
In CNNs this depth would be constant, which mitigates the problem of forgetting tokens seen in a less recent past.

% cnn outro
A review by~\citet{young2018recent} argues that CNNs are suited for specific NLP tasks, but when data is scarce they are not effective.
Foundational problems with CNNs are that they are unable to model long-distance contextual information and to preserve sequential order~\citep{young2018recent}.
The former implies that CNNs are not well suited for question answering tasks such as SQuAD~\citep{rajpurkar2018}.

\subsection{ELMo}
\label{subsec:elmo}
Word embeddings generated by well-known models such as Word2vec~\citep{mikolov2013distributed} and GloVe~\citep{pennington2014} do not take context into account when determining word representations.
For ELMo ``each token is assigned a representation that is a function of the entire input sentence''~\citep{peters2018}.
This is done by using a bidirectional LSTM.
Word embeddings are used only to map words to vector representations.
To improve accuracy further, compared to traditional embeddings, the authors advise to use `the deep internals of the network'~\citep{peters2018}.
These internals can be used for downstream models, also known as transfer learning.
For example, the authors show that word sense disambiguation tasks are captured by higher level LSTM states.
Part-of-speech tagging or named entity recognition are captured by lower level states.
ELMo is not the first system to use context, but was obtaining state-of-the-art empirical results on multiple non-trivial tasks at the time of publication.
Another reason for the good results is that the system is character based.
Word based systems cannot generate an embedding for a word they have not seen during training (out-of-vocabulary tokens).
In character based systems morphological clues can be used to guess the meaning of the out-of-vocabulary words.
The system has quickly become very popular.
Reasons for this seem to be the high accuracy, that the system generalizes well, and that it is integrated into the AllenNLP open-source NLP library created by~\citet{gardner2017}.

\subsection{Transformers}
\label{subsec:transformers}
The main issue in the recurrent approaches is that distant information needs to pass through all the intermediate states.
In the basic RNN for each state all the information is updated, causing less recent information to gradually disappear.
Gated recurrent architectures (GRUs and LSTMs) reduce this problem by being more selective when viewing or changing information.
Transformer networks allow the model to look at previous inputs instead of previous states~\citep{vaswani2017attention}.
For example, suppose we are translating `impounded' in the following sentence pair from the WMT'14 English-German dataset.
\begin{center}
    The motorcycle was seized and \underline{impounded} for three months.\\[3mm]
    Das Motorrad wurde sichergestellt und f\"ur drei Monate \underline{beschlagnahmt}.
\end{center}
Suppose the system has correctly predicted the German sentence up to and including `Monate'.
The next step is to predict `beschlagnahmt'.
To do this the system needs mainly information about the word `impounded'.
Gated recurrent architectures learn to look at the previous state in such a way that the attention is focused on `impounded'.
This requires the information of the word to not been overwritten during execution.

Transformers evade this overwriting problem by allowing the system to see all $d$ previous words, where $d$ is 1024 for the biggest model.
The only thing the transformer then needs to learn is where to focus its attention.
The information of all the previous words is stored in an array of word vectors (a tensor).
To apply focus to parts of this tensor the model learns to put a mask over the tensor.
In the mask hidden items are multiplied by infinity~\citep{vaswani2017attention}.
One drawback of this architecture is the required computational power.
Suppose we only need one word from the $d$ previous words.
The mask will hide $d-1$ words.
This still requires to multiply the masked word vectors by infinity.
Google argues that this is not really an issue since matrix multiplication code is highly optimized and graphic processing units (GPUs) and tensor processing units (TPUs) exist.
So, the model can relate any dependency in constant time when the range is lower than $d$.
This in contrast to recurrent layers which are in linear time.
When the sequence length is greater than $d$, computations will require more than linear time~\citep{vaswani2017attention}.

Another benefit of the transformers are that self-attention visualisations can more easily be done than in recurrent architectures.
By self-attention the authors refer to attention that is used to generate context aware word representations.
An example of a tranformer model correctly applying coreference resolution is shown in figure~\ref{fig:coreference_resolution}.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/coreference_resolution.png}
    \end{center}
    \caption{Example of transformer encoder self-attention distribution~\citep{uszkoreit2017}.}
    \label{fig:coreference_resolution}
\end{figure}

\iffalse
\subsection{BERT}
\label{subsec:bert}
WordPiece tokenization, as published by~\citet{wu2016}, is used.
They argue that the tokenization ``provides a good balance between the flexibility of `character'-delimited models and the efficiency of `word'-delimited models''.
\fi