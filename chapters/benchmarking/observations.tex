\section{Observations}
\label{sec:observations}

\textsc{bench} requires some further improvements, as explained in Section~\ref{subsec:benchmarking_system}.
Tool design was guided by the methodology as presented by~\citet{braun2017}.
Section~\ref{subsec:methodology} points out some observations which could improve the the proposed methodology.

\subsection{Benchmarking system}
\label{subsec:benchmarking_system}
% data, dependencies, payments, depending on use case, also company preferences matter
While developing the benchmarking system, it is observed that the current implementation requires further work.
The observations are as follows.
Few datasets are publicly available.
Current datasets are either small (AskUbuntu, Chatbot, WebApplications) or domain specific (Snips2017).
Rasa, for example, uses `a dozen different datasets'~\citep{nichol2018vectors} for verification.
One possible explanation for not sharing these datasets is the sensitive nature of natural language.
Another is that they simply not share it to have a competitive advantage.
We also observe that dependencies require the product to be continuously maintained.
The dependencies are mainly in the form of APIs.
APIs are meant to be used by software and will therefore not change often.
However, eventually they will do.
So, eventually the benchmarking software needs to be updated.
Another problem is that the services which offer APIs are not free to use.
For each system to be evaluated we need a separate API key.
The owner of the benchmarking tool can decide to offer paid keys or let users set keys manually.
The former might be possible by negotiations with NLU service providers which are incentivized by selling their product.
The latter requires users to have an account for each service.
A closed-source solution is Intento (\url{https://inten.to}).
One can send data to the site via their API and they will run a benchmark on various services for a given task.
Their `catalog' contains machine translation, intent detection, sentiment analysis, text classification, dictionaries, image tagging, optical character recognition and speech-to-text.
The final observation for \textsc{bench} is as follows.
When choosing a system not only the performance matters.
One reason for this is that accuracies are volatile.
If companies would base their decision solely on `the highest accuracy' then they would need to change system each month.
Since companies cannot spend their time constantly switching systems they should also take other factors into account.
These factors can include pricing, memory usage, classification speed, privacy (whether open-source) and in-house API preferences.

% summary
In summary, we define the following requirements.
\begin{itemize}
    \item The tool should be continuously maintained,
    \item offer an API key for each service, or let users add their own keys,
    \item report more information than just accuracy statistics,
    \item report evaluation statistics over multiple training runs, \hspace*{3mm} and
    \item include bigger and more varied datasets.
\end{itemize}

\subsection{Methodology}
\label{subsec:methodology}
Creating a benchmarking tool has resulted in more insight into intent classification.
This helped in identifying improvements for the methodology proposed by~\citet{braun2017}.
As discussed in Section~\ref{subsec:available_datasets} falling back to `None' or a random intent changes $\text{F}_1$ score.
In the paper Chatbot does not have a `None' intent, while WebApplications and AskUbuntu do.
Furthermore, drawn conclusions about some system being more accurate than others seems insubstantial.
The conclusion that accuracy of some system depends on the domain seems convincing, but is poorly grounded.
Reason for this is that both conclusions are based on the $\text{F}_1$ score.

% average
In this paper, the $\text{F}_1$ score is calculated using micro $\text{F}_1$ score.
Such a score does not take classes of different size, so called class imbalances, into account.
This is combined with a situation where intents and entities are given the same weight.
For WebApplications there are in total 74 labeled intents and 151 labeled entities.
AskUbuntu contains 128 labeled intents and 123 labeled entities.
So, when using micro $\text{F}_1$ on AskUbuntu the score is based somewhat equally on intents and entities.
For WebApplications the score is based for about one thirds on intents and two thirds on entities.
This could mean that some system has scored significantly better that others simply because it labels entities in WebApplications particularly well.
Another reason which makes this score odd is that users interested in either intent or entity classification are not well informed.
Better seems to be using macro or weighted $\text{F}_1$ and reporting separate intent and entity scores.
Macro averaging (and by that weighted averaging) is better suited to ``get a sense of effectiveness on small classes''\citep{schutze2008introduction}.
Rasa, for example, uses the weighted average in their evaluation according to their code on Github.

% non-deterministic probalistic models, showing detailed information is misleading
Another reason to distrust presented $\text{F}_1$ scores is the probabilistic nature of neural networks.
Although inference (classification) is deterministic, training is not.
During training models often start with random weights.
Random initializations can move into different local minima for the same training data.
This could change the inference results.
During benchmarking this effect has been observed for Rasa using the spaCy back-end.
According to a mail from the main author Rasa 0.5 with the MITIE back-end is used for the results described in the paper.
The MITIE back-end has not shown to change accuracy after re-training the model.
Microsoft LUIS and Google Dialogflow also did not show a change in accuracy after re-training.
So, it could be that all the systems in the benchmark were deterministic.
Still, the problem of not considering the possibility of changing accuracy persists.

