\section{Systems}
\label{sec:systems}

\subsection{Rasa}
\label{subsec:rasa}
% introduction
Rasa (\url{https://rasa.com/}) is an open-source system allowing users to build conversational agents.
The systems consists of two parts, namely \textsc{rasa\_nlu} and \textsc{rasa\_core}.
The former classifies sentences and sub-sentences.
To train the system users can specify (hierarchical) intents, synonyms and regexes.
Hierarchical intents is a recent addition which allows the system to extract multiple intents from a sentence.
For example, it can extract `hi+book\_ticket' from
``Good morning.
Can I order a ticket to London please?''.
The system is actively used in production.
As a result the code is well documented and stable.

% dialogue management
\textsc{rasa\_core} aims to handle dialogue management.
This is an extension on the classifiers of \textsc{rasa\_nlu} which aims to understand text in context.
Also, it can be used to specify conversation flow.
This part remains one of the most difficult problems for conversational agents.
Humans tend to switch rapidly between topics in conversations.
For example, suppose one ticket order conversation flow contains six questions to be answered by the customer.
Customers expect to be able to switch topic during each one of these questions and then return to the flow.
Enabling this behaviour via state machines or flowcharts is cumbersome, because the number of transitions tends to grow quickly.
One of the Rasa solutions is applying machine learning to let developers train dialog flows interactively.

% using system
Rasa allows the system to be used as API and `Python API'.
The Python API is the most efficient.
Here users install \textsc{rasa\_nlu} in their programming environment and call functions directly.
Depending on the used configuration a selection of dependencies have to be installed.
Since Rasa is called from Python it is not able to maintain a state.
The regular API advises to spin up a Docker container.
This is less efficient, but more modular.
Containers are published to Docker Hub by Rasa.
Users can pull these for free and use the newest stable configuration of choice.

% back-ends or pipelines
Configurations are defined as a pipeline.
Pipelines specify what components should be applied to sentences and in what order.
Typical pipelines contain at least a tokenizer followed by some classifier.
Pipelines are meant to be modified easily and are specified in yaml.
In practise default pipelines often suffice for end-users.
A back-end refers to the used intent classifier in some pipeline, for example `tensorflow'.
Three back-ends are offered by Rasa via Docker Hub, namely \textsc{rasa-mitie}, \textsc{rasa-spacy} and \textsc{rasa-tensorflow}.
\textsc{rasa-mitie} is the oldest and depreciated.
% note-to-self: do not change MITIE to \textsc{MITIE} or \textsc{mitie} or whatever.
Training MITIE (\url{https://github.com/mit-nlp/MITIE}) takes at least a few minutes for small datasets.
This is caused by the fact that it is tuning hyperparameters during training.
On two computers used for this thesis the Docker Hub image occasionally hangs on various datasets.
\textsc{rasa-spacy} is the successor of MITIE and, unsurprisingly, based on spaCy (\url{https://spacy.io}).
In 2015 spaCy was in the top 1\% for accuracy and the fastest syntactic parser~\citep{choi2015depends}.
spaCy (and by that \textsc{rasa-spacy}) uses a language model to parse text.
It includes seven language models for which English, Spanish and French include word vectors.
The multilingual model supports only named entities.
Unlike the other two back-ends \textsc{rasa-tensorflow} is not based on a pre-trained language model.
This is like classifying sentences in an unfamiliar language (say Chinese) after only seeing some examples.
Rasa advises to use this back-end when training data contains more than 1000 training examples.
The benefit of \textsc{rasa-tensorflow} is that the back-end is language independent and can handle domain specific data and hierarchical intents.

\subsection{DeepPavlov}
\label{subsec:deeppavlov}
DeepPavlov~\citep{burtsev2018} is similar to Rasa.
Unlike Rasa, DeepPavlov aims to aid researchers in development of new techniques for conversational agents.
Being a newer system than Rasa and aimed at researchers the system is not yet production ready.
The system does only provide a Python API, requiring Python 3.6.
One claimed benefit of the system is that they do not export machine learning components from other systems.
A reason why the system is not well suited for production is that pipelines can download information.
This means that a generic Docker needs to download many megabytes of data for each time the Docker is started.
Manually defining new Dockers holding this information is possible, but does require some knowledge about Docker and some time to set it up.
For users who want to use few training examples pre-trained models are necessary.
DeepPavlov by default includes DSTC 2, Wikipedia, Reddit, RuWiki+Lenta and ELMo embeddings.

% cloud services
\subsection{Cloud services}
\label{subsec:cloud_services}
% conversational agents
Cloud service providers and various small companies (start-ups) provide APIs for conversational agents.
Functionality differs per provider, but in the basis they all over the same features.
Naming conventions do not seem to exists.
For example, Rasa calls intent classification training examples \textit{utterances}, while Google Dialogflow calls them \textit{training phrases}.
Via the web interface or API examples can be sent to a server and the system can be configured.
Configurations specify the example utterances, dialog flows, how to classify entities (used for slot filling) and input language.
At some point the server will use the provided examples to train the model.
This takes a few seconds.
% This makes sense, since users do not want to wait and computations cost money.
Settings for the dialogs and APIs are located somewhere in the complete cloud offerings.
An extension on the intent classification and slot filling described above is using knowledge bases.
When looking at IBM Watson a document needs to be uploaded and annotated by humans.
This is an example of a structured knowledge base.

% list of further natural language understanding providers
In the period that IBM Watson won the Jeopardy quiz a lot of math and reasoning was required to create NLP systems.
Nowadays, training a competitive neural network for natural language processing is relatively easy.
It takes a PhD candidate a few months~\citep{manning2017lectures}.
This results in a lot of companies providing natural language processing services.
An in-depth analysis of all services is left out.
The following is a non-exhaustive list.
Some offer full conversational agent capabilities, while others focus on natural language understanding.
\begin{description}
    \item [Watson Assistant (\url{https://www.ibm.com})] is a conversational agent by IBM.
    \item [Dialogflow (\url{https://dialogflow.com})] is a conversational agent by Google.
    \item [Lex (\url{https://aws.amazon.com/lex})] is a variant created by Amazon.
    \item [LUIS (\url{https://www.luis.ai})] is the conversational agent created by Microsoft.
    \item [wit.ai (\url{https://wit.ai})] can be used for chatbots and is acquired by Facebook.
    The system is free to use, but Wit is allowed to use the data sent to their servers.
    \item [Deep Text (\url{https://deeptext.ir})] provides sentiment analysis, text classification, named-entity recognition and more.
    \item [Lexalytics (\url{https://www.lexalytics.com})] provides categorization, named entity recognition, sentiment analysis and more.
    \item [Pat (\url{https://pat.ai})] has as goal to humanize AI and provides some conversational agent services.
    \item [kore.ai (\url{https://kore.ai})] focus lies on intent classification and entity extraction with as goal to replace graphical user interfaces with chatbots.
\end{description}
Sixteen more are listed by~\citet{dale2018text}.
