\section{Tool and results}
\label{sec:bench}
% intro
The benchmarking tool is called \textsc{bench} and available on Github\footnote{\url{https://github.com/rikhuijzer/bench}}.
Its goal is to be a reproducible benchmarking tool for intent classification and named-entity recognition.
Reproducible means that anyone can clone and run the code to reproduce the results presented in this thesis.
The code is written in Python, since it is the default choice for machine learning.
Python is conceived as a object-oriented language.
Over time it has included more and more functional programming ideas.
The code in this project will aim to be adhering to functional programming.
Reasons are pedagogic value, improved modularity, expressiveness, ease of testing, and brevity.
Some general notes on functional programming in Python are listed in Appendix~\ref{ch:fp}.

% final code remarks on fp and imports
The functional programming constraints for the project are that we do not define any new classes.
Specifically, we do not use the class keyword.
Exceptions being NamedTuples and Enums.
The code prefers returning iterators over collections, the reason for this is explained in Appendix~\ref{ch:demonstration}.
A final remark is about the imports.
When importing, an attempt is made to explicitly import using `from $<$module$>$ import $<$class$>$'.
When more implicit imports are used `import $<$module$>$' this is can have multiple causes.
It is either caused by the appearance of circular imports, by the fact that some names are too common or to avoid reader confusion.
An example for the latter are the types defined in \textsc{src.typ}.
The names are quite generic and could cause name clashing or confusion when imported explicitly.

\subsection{Overview}
\label{subsec:overview}
Since the code does not contain classes, the high-level overview is simply a tree-like structure.
This is analogous with a book, where subsections are contained in sections and sections are contained in chapters.
In the code small functions are called by larger functions and these larger functions are called by even larger functions.
For an overview this idea can be generalized to modules.
An overview for the modules of \textsc{bench} is roughly as follows.
The `.py' suffix is omitted for all elements in the tree.
Tests are also omitted.

\begin{itemize}
    \item \textsc{bench}
    \begin{itemize}
        \item \textsc{src.utils}
        \item \textsc{src.typ}
        \item \textsc{src.dataset}
        \begin{itemize}
            \item \textsc{src.datasets.corpora}
            \item \textsc{src.datasets.snips}
        \end{itemize}
        \item \textsc{src.system}
        \begin{itemize}
            \item \textsc{src.systems.amazon\_lex}
            \item \textsc{src.systems.deeppavlov}
            \item \textsc{src.systems.dialogflow}
            \item \textsc{src.systems.mock}
            \item \textsc{src.systems.rasa}
            \item \textsc{src.systems.watson}
        \end{itemize}
        \item \textsc{src.evaluate}
        \begin{itemize}
            \item \textsc{src.results}
        \end{itemize}
    \end{itemize}
\end{itemize}

Some generic functions are listed in \textsc{src.utils} and used through the entire project.
The project makes use of type hints as introduced in Python 2.7.
All NamedTuples or `types' are defined in \textsc{src.typ}.
An overview of the most important types is depicted in Figure~\ref{fig:types}.
These types also contains enumerables or `Enums'.
These are used in cases where function behaviour depends on some parameter having a fixed set of options.
Alternatively one could use strings for these cases depending on user-preference.
Notable is the usage of \textsc{System}, and by that the usage of \textsc{Corpus}, in \textsc{Query}, \textsc{SystemCorpus}, \textsc{Classification} and \textsc{F1Score}.
This is caused by the fact that external systems (for example, DialogFlow) have a certain state which needs to be passed through many functions.
This state could be that the system has not yet seen the dataset, resulting in \textsc{Corpus.EMPTY}, or the system has trained on AskUbuntu, which results in \textsc{Corpus.ASKUBUNTU}.
In, for example, \textsc{Classification} this is used to let some evaluation function know context for an input sentence.
This context includes from what dataset the sentence came and what system has classified the sentence.

% Define block styles
\tikzstyle{block} = [rectangle, draw, fill=blue!20,
    text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm, minimum height=2em]
\tikzstyle{small_block} = [rectangle, draw, fill=blue!20,
    text width=5em, text centered, rounded corners, minimum height=4em]

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[node distance = 3cm, auto]
        % Place nodes
        \node [small_block] (0) {System};
        \node [small_block, left of=0] (10) {Corpus};
        \node [small_block, below of=0] (1) {System-\\Corpus};
        \node [small_block, left of=1] (2) {Query};
        \node [small_block, right of=0] (3) {Response};
        \node [small_block, right of=1] (4) {Classi-\\fication};
        \node [small_block, right of=4] (5) {F1Score};

        \node [small_block, below of=1] (6) {CSVStats};
        \node [small_block, left of=6] (7) {CSVIntent};
        \node [small_block, right of=6] (8) {CSVEntity};
        \node [small_block, right of=8] (9) {CSV};

        \path [line] (10) -- (0);
        \path [line] (0) -- (1);
        \path [line] (10) -- (1);
        \path [line] (0) -- (2);
        % to classification
        \path [line] (0) -- (4);
        \path [line] (3) -- (4);
        % to f1 score
        \path [line] (0) -- (5);
    \end{tikzpicture}
    \caption{Overview of most important type classes (NamedTuples and Enums) and their relations in \textsc{bench}.
    Here a line from $A$ to $B$ means that $B$ is a type which includes $A$ ($B$ extends $A$).}
    \label{fig:types}
\end{figure}

The real work of the project is done by \textsc{src.dataset}, \textsc{src.system} and \textsc{src.evaluate}, as shown in Figure~\ref{fig:flowchart}.
`Dataset' takes input files and converts them to an internal representation as defined by \textsc{src.typ}.
Input files here denote the original dataset files as created by the dataset publishers.
For the internal representation a Rasa Message\footnote{\textsc{rasa\_nlu.training\_data.message.Message}} is used.
The benefit of this is that it avoids defining the same structure and that it can be used in combination with Rasa code.
For example, \textsc{src.dataset.convert\_message\_to\_annotated\_str} uses Rasa code to print the internal data representation as a sentence in Markdown format (Section~\ref{subsec:format}).
Next, the data reaches \textsc{src.system}.
Here it is passed to the system under consideration, either in training or prediction mode.
For the predictions this is done by finding out which function can convert \textsc{src.typ.Query} to \textsc{src.typ.Response}.
When, for example, Rasa is under consideration the function \textsc{src.systems.rasa.get\_response} is called.
DeepPavlov would be handled by \textsc{src.systems.deeppavlov.get\_response}.
PyCharm is known to have the best type inference for Python.
The IDE is not yet able to infer function type for a function mapping, even when all functions have the same input and output type.
A workaround is to manually define the type of the function returned by the mapping as \verb|func: Callable[[tp.Query], tp.Response] =| $\cdots$.
\textsc{src.evaluate} takes all responses \textsc{tp.Response} evaluates the performance of the system under consideration.
Printing $\text{F}_1$ score is a matter of three functions and about a dozen lines of code.
At one point more advanced logging has been included which is responsible for the other 12 functions and 110 lines of code.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[node distance = 2cm, auto]
        % Place nodes
        \node [block] (srcdataset) {\textsc{src.dataset}};
        \node [cloud, above of=srcdataset] (dataset) {dataset};
        \node [block, below of=srcdataset] (srcsystem) {\textsc{src.system}};
        \node [cloud, right of=srcsystem] (system) {system};
        \node [block, below of=srcsystem] (srcevaluate) {\textsc{src.\\evaluate}};
        \node [cloud, below of=srcevaluate] (score) {\fone score};
        % Draw edges
        \path [line] (dataset) -- (srcdataset);
        \path [line] (srcdataset) -- (srcsystem);
        \path [line] (srcsystem) -- (srcevaluate);
        \path [line] (srcevaluate) -- (score);
        \path [line] (srcsystem) -- (system);
        \path [line] (system) -- (srcsystem);
    \end{tikzpicture}
    \caption{Diagram showing the dataflow in \textsc{bench}.}
    \label{fig:flowchart}
\end{figure}

\subsection{Benchmark results}
\label{subsec:benchmark_results}
This section presents the benchmark results for intent classification using \fone scoring with micro averaging.
\fone formulas are listed in Section~\ref{subsec:f1_score}.
An explanation for the differences in averages for the \fone score is presented in Section~\ref{subsec:methodology}.
Here micro \fone scores are used to allow comparing results with~\citet{braun2017}.
The results are listed in Table~\ref{tab:benchmark_comparison}.

\begin{table}[!htbp]
    \centering
    \begin{tabular}{l c c c c}
        \textbf{System} & \textbf{Source} & \textbf{AskUbuntu} & \textbf{Chatbot} & \textbf{WebApplications} \\
        \hline
        Rasa:0.5-mitie & see Table~\ref{tab:recalculations_rasa} & 0.862 & 0.981 & 0.746 \\
        Microsoft LUIS & see Table~\ref{tab:recalculations_luis} & 0.899 & 0.981 & 0.814 \\
        Watson Conversation (2017) & see Table~\ref{tab:recalculations_watson} & 0.917 & 0.972 & 0.831 \\
        Rasa:0.13.7-mitie & \textsc{bench} & 0.881 & & 0.763 \\
        Rasa:0.13.8-spacy & \textsc{bench} & 0.853 & 0.981 & 0.627 \\
        Watson Conversation (2018) & \textsc{bench} & 0.881 & 0.934 & 0.831 \\
        \hline
    \end{tabular}
    \caption{Micro $\text{F}_1$ scores for intent classification.
    One score is missing due to a bug in \textsc{bench}.}
    \label{tab:benchmark_comparison}
\end{table}

The paper remarks that ``For our two corpora, LUIS showed the best results, however, the open source alternative RASA could achieve similar results''~\citep{braun2017}.
When considering only intents this does not hold.
Watson Conversation has very similar results, and in fact slightly higher scores on two out of three datasets.
The MITIE back-end outperforms the spaCy back-end in terms of accuracy.
This would not support the choice of Rasa to depreciate MITIE.
It is expected to be caused by the facts that training MITIE takes more time than spaCy and MITIE tends to freeze during training.
Interesting to see is that the accuracy for Watson Conversation has dropped.
The cause can only be guessed since IBM does not provide information about the Watson back-end.
It could be that the calculations for \textsc{bench} and the paper differ.
Alternatively it could be that the back-end for Watson has changed.
The datasets under consideration are small, so it might be that Watson has chosen a back-end better suited for large datasets.
Note that IBM is aimed at large companies.
These companies have the resources for creating lots of training examples.