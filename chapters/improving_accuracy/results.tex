\section{Results}
\label{sec:results}

% less text is allowed for this section since it makes sense
Experiments are conducted on the AskUbuntu, Webapplications, Chatbot and SNIPS2017 dataset as introduced in Section~\ref{subsec:available_datasets}.
Comparisons are made for a fixed number of steps (or equivalently epochs).
The reason for this is that intermediate results are not easily reported for the BERT model as explained in Section~\ref{subsec:training}.
The number of steps to be used for training is based on a guess on what should be enough.
Validating whether the model should have been trained for more epochs can be done by looking at the loss reported in Appendix~\ref{ch:runs} for different number of steps.
For each dataset various runs for BERT are executed.
During one run only intents are shown to the system and accuracy is measured for intents.
Another run only shows entities and measures intents.
A third run shows the system intents and entities and measures both.
These methods are denoted as separate or joint.
It is expected that the joint training increases accuracy for both intent and entity classifications.
The reason for this is that the model sees more varied data and hence should be able to more easily find a good internal representation of the data.
Results are listed in Table~\ref{tab:runs_scores}.
Further experiments on the near zero score on Snips2017 separate intent are located in Section~\ref{sec:additional_experiments}.

% notes on runs
Note that separate training consists of two runs, and hence ran twice as many epochs.
This seems fair, since intent or entity improvements which require twice as many training steps are not interesting for expensive models such as BERT.
As a baseline Rasa 0.13.8 with the Spacy pipeline is used.
At the time of writing only intent classification is implemented using the benchmark code, so entity scores are missing.
Omitting scores for other systems has been deliberate.
The table is merely meant to support that joint training is feasible.
A final remark is that the scores have been rounded to two decimals.
The number of epochs is calculated by taking number of training steps times training batch size and dividing by number of training examples.
% todo: list number of intents and entities per dataset in table, like discussed in methodology
The training time for 600 steps is around 10 minutes, for 6000 steps it is around 20 minutes.
The reason for the relatively small increase in training time is that most time is spent on training preparations and transferring model checkpoints between TPU and cloud storage.

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l c c}
        \textbf{Dataset}    & \textbf{Steps}  & \textbf{Batch size} & \textbf{Epochs}  & \textbf{Method}   & \textbf{Intent}  & \textbf{Entity}\\
        \hline
        WebApps             & & & & Rasa & $0.67 \pm 0.04$\\
                            & 600 (twice) & 32 & 640 (twice) & separate & $0.72 \pm 0.03$ & $0.81 \pm 0.01$ \\
                            & 600 & 32 & 640 & joint & $0.76 \pm 0.07$ & $0.82 \pm 0.01$ \\
        \hline
        AskUbuntu           & & & & Rasa & $0.84 \pm 0.00$ \\
                            & 600 (twice) & 32 & 362 (twice) & separate & $0.82 \pm 0.05$ & $0.81 \pm 0.01$\\
                            & 600 & 32 & 362 & joint & $0.87 \pm 0.01$ & $0.83 \pm 0.00$\\
        \hline
        Chatbot             & & & & Rasa & $0.98 \pm 0.00$ \\
                            & 600 (twice) & 32 & 192 (twice) & separate & $0.84 \pm 0.21$ & $0.76 \pm 0.00$\\
                            & 600 & 32 & 192 & joint & $0.98 \pm 0.00$ & $0.79 \pm 0.00$\\
        \hline
        Snips2017           & & & & Rasa & $0.99 \pm 0.00$\\
                            & 6000 (twice) & 32 & 91 (twice) & separate & $0.04 \pm 0.00$ & $0.84 \pm 0.00$\\
                            & 6000 & 32 & 91 & joint & $0.98 \pm 0.02$ & $0.86 \pm 0.00$\\
        \hline
    \end{tabular}
    \caption{Weighted $\text{F}_1$ accuracy scores ($\text{mean} \: \pm \: \text{standard deviation}$, over three runs) for separate and joint training on four datasets.
    Details are listed in Appendix~\ref{ch:runs}.}
    \label{tab:runs_scores}
\end{table}

% result analysis
From the results it can be concluded that joint training is feasible.
Joint training increases the named-entity recognition accuracy for each dataset.
For intent classification joint training significantly increases accuracy compared to separate training.
Compared to the \textsc{rasa-spacy} baseline it performs better or similar on all the small datasets (WebApplications, AskUbuntu, Chatbot).
The accuracy is slightly lower for Snips2017.
The model accuracy will be near zero when training on intents separately for SNIPS2017 and the model accuracy will vary for Chatbot.
For Snips2017 it has been found that lowering the step size does not solve this problem, as listed in Table~\ref{tab:runs_snips}.
%This suggests that the gradient descent does not converge because the step size is too big.
%A bigger batch size means a more stable error gradient.
One reason for the poor performance on intent classification only is that the model get stuck in a local minimum.
% It might be that this caused the model to get stuck in a local minimum.
The difference with joint training is that the joint training data is much more varied.
Say a typical sentence contains 12 tokens.
Then a joint training batch of size 32 will contain about 3 tokens related to intents and 29 related to entities.
For an intent training batch of size 32 it will contain 32 tokens related to intents.
Hence, the data for the joint training is much more complex.
This seems to indicate that the joint training forces the model to learn a more complex representation.

% closing remarks
An important thing to note about the results is that the datasets are very small.
One would expect that the large BERT model is better suited for datasets which contain more training examples.
Furthermore, the experiments are based on a naive implementation.
For intent the model as defined by~\citet{devlin2018github} is used.
For named-entity recognition and joint training the model by~\citet{kyzhouhzau2018ner} is used.
Not only the batch size but also other hyperparameters can be tuned for better results.
Training has used a fixed number of steps or epochs.
It might be that more epochs give higher accuracies.
On the other hand it might also be that less epochs correspond to similar accuracies in less training time.
Other interesting hyperparameters are \textit{max\_seq\_length} and \textit{learning\_rate}.
Lowering the former to the expected maximum number of tokens in sentences reduces training and inference time.

Observe that joint training generalizes to sequential labeling and sentence classification.
In other words, any combination of tasks where sentences are classified as well as parts of the sentence and these tasks are correlated.
The tasks are expected to be correlated for any real-world NLP dataset.

\section{Implementation improvements}
\label{sec:implementation_improvements}
The current implementation is a proof-of-concept.
The loss function for joint training needs to be reconsidered.
Currently the importance of intent and all named-entities is the same, meaning one error in a intent or in a named-entity weights the same.
It might be better for the model to have a higher importance for the intents, since the intent contains information about the entire sentence.
Further tests are needed to see whether the increase in accuracy is significant.
Comparing NER is currently non-trivial since the accuracy calculation is based on the tokenizer used by BERT.
The BERT tokenizer (\textsc{FullTokenizer}) is based on the vocabulary file corresponding to the pre-trained BERT model.
So, when trying to compare the score with, for example, Rasa one has to use the same tokenizer and vocabulary file to calculate the Rasa score.
Better would be to use a more standard tokenizer as provided by NLTK~\citep{bird2004nltk}.
Another thing needed to determine the significance of the accuracy improvements is more and more varied datasets.
No dataset of around a few hundred distinct training examples has been evaluated, while this seems realistic for production use-cases.
One could merge AskUbuntu, WebApplications and Chatbot, but this would be an unrealistic dataset.
It would be a dataset where two thirds is related to software questions and one third is related to public transport information.
Tests should also be conducted using the other models such as \bert{multilingual} and \bert{base} (cased).
The authors~\citep{devlin2018} use English models to get SOTA results, this suggests that \bert{multilingual} performs worse.
It is unknown how big the difference between the two is.
Another improvement would be setting up fine-tuning such that TensorBoard visualisations (like Figure~\ref{fig:tensorboard}) are possible.
This allows users to get a better insight into the model training.
Lastly, the code is in need of refactoring and validation.
For example, the NER code has a distinction between X and O.
O is used by the BIO annotation standard and X is used for positions which are not used (since the sentence is shorter than the maximum sequence length).
The model in its current implementation does use X to classify tokens.

% less important improvements
Less important improvements are using conditional code to fix I-\textsc{entity} statements appearing without a B-\textsc{entity}.
This constraint is enforced by the BIO annotation standard, where `intermediate' statements are preceded by `begin' statements.
Also, for production use it might be beneficial to do some hyperparameter tuning.

% use more standard tokenization for determining accuracy score, this one is vocab based tokenization.

% todo: general suggestions such as validate training data (find outliers and duplicates)
% todo: check accuracy on datasets having more than intent and entity, but also part of speech tagging or whatever