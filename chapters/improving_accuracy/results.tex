\section{Results}
\label{sec:results}

% less text is allowed for this section since it makes sense
Experiments are conducted on the AskUbuntu, Webapplications, Chatbot and SNIPS2017 dataset as introduced in section~\ref{subsec:available_datasets}.
Comparisons are made for a fixed number of steps (or equivalently epochs).
The reason for this is that intermediate results are not easily reported for the BERT model as explained in section~\ref{subsec:training}.
The number of steps to be used for training is based on a guess on what should be enough.
For each dataset various runs for BERT are executed.
During one run only intents are shown to the system and accuracy is measured for intents.
Another run only shows entities and measures intents.
A third run shows the system intents and entities and measures both.
These methods are denoted as separate or joint.
It is expected that the joint training increases accuracy for both intent and entity classifications.
The reason for this is that the model sees more varied data and hence should be able to more easily find a good internal representation of the data.
Results are listed in table~\ref{tab:runs_scores}.

% notes on runs
Note that separate training consists of two runs, and hence ran twice as many epochs.
This seems fair, since intent or entity improvements which require twice as many training steps are not interesting for expensive models such as BERT.
As a baseline Rasa 0.13.8 with the Spacy pipeline is used.
Only intent classification is possible using the benchmark code, so entity scores are missing.
Omitting scores for other systems has been deliberate.
The table is merely meant to support that joint training is feasible.
A final remark is that the scores have been rounded to two decimals.
The reason for this is that results vary between runs and for changes to hyperparameters.
To avoid falsely reporting perfect scores (having accuracy of 1.0), all intent and entity numbers are rounded down.
The number of epochs is calculated by taking number of training steps times training batch size and dividing by number of training examples.
% todo: list number of intents and entities per dataset in table, like discussed in methodology
The training time for each model is around 10 minutes.
Running time differences between runs are small, since most time is spent on training preparations and transferring model checkpoints between TPU and cloud storage.

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l c c}
        \textbf{Dataset}    & \textbf{Steps}  & \textbf{Batch size} & \textbf{Epochs}  & \textbf{Method}   & \textbf{Intent}  & \textbf{Entity}\\
        \hline
        AskUbuntu           & & & & Rasa & 0.83 \\
                            & 250 (twice) & 32 & 151 (twice) &  separate & 0.74 & 0.99\\
                            & 250 & 32 & 151 & joint & 0.98 & 0.79\\
        \hline
        WebApplications     & & & & Rasa & 0.67\\
                            & 250 (twice) & 32 & 267 (twice) & separate & 0.00 & 0.79\\
                            & 250 & 32 & 267 & joint & 0.65 & 0.81\\
                            & 1000 (twice) & 8 & 267 (twice) & separate & 0.72 & 0.79\\
                            & 1000 & 8 & 267 & joint & 0.53 & 0.80\\
        \hline
        Chatbot             & & & & Rasa & 0.98 \\
                            & 250 (twice) & 16 & 40 (twice) & separate & 0.99 & 0.74\\
                            & 250 & 16 & 40 & joint & 0.98 & 0.79\\
        \hline
        Snips2017           & & & & Rasa & 0.99\\
                            & 1500 (twice) & 32 & 22.9 (twice) & separate & 0.03 & 0.83\\
                            & 1500 & 32 & 22.9 & joint & 0.97 & 0.85\\
        \hline
    \end{tabular}
    \caption{Weighted f1 accuracy scores for separate and joint training on four datasets.}
    \label{tab:runs_scores}
\end{table}

The code to reproduce the results the logs (including predictions) can be found at the links provided in table~\ref{tab:runs_urls}.
\begin{table}[htbp]
    \centering
    \begin{tabular}{l l}
        \textbf{Dataset}    & \textbf{Url}\\
        AskUbuntu           & \url{https://github.com/rikhuijzer/improv/tree/master/runs/askubuntu}\\
        WebApps             & \url{https://github.com/rikhuijzer/improv/tree/master/runs/webapplications} \\
        Chatbot             & \url{https://github.com/rikhuijzer/improv/tree/master/runs/chatbot} \\
        Snips2017           & \url{https://github.com/rikhuijzer/improv/tree/master/runs/snips2017} \\
    \end{tabular}
    \caption{Hyperlinks for Rasa and BERT run information.}
    \label{tab:runs_urls}
\end{table}

% result analysis
From the results it can be concluded that joint training is feasible.
The model will not learn anything when training on intents separately for SNIPS2017 and WebApplications.
For WebApplications and Snips2017 it has been found that lowering the step size can solve this problem.
This suggests that the gradient descent does not converge because the step size is too big.
A bigger batch size means a more stable error gradient.
It might be that this caused the model to get stuck in a local minimum.
Specifically, the difference with joint training is that the joint training data is much more varied.
Say a typical sentence contains 12 tokens.
Then a joint training batch of size 32 will contain about 3 tokens related to intents and 29 related to entities.
For an intent training batch of size 32 it will contain 32 tokens related to intents.
Hence, the data for the joint training is much more complex.
This seems to indicate that the joint training forces the model to learn a more complex representation.

% closing remarks
An important thing to note about the results is that the datasets are very small.
One would expect that the large BERT model is better suited for datasets which contain more training examples.
Furthermore, the experiments are based on a naive implementation.
Not only the batch size but also other hyperparameters can be tuned for better results.
Training has used a fixed number of steps or epochs.
It might be that more epochs give higher accuracies.
On the other hand it might also be that less epochs correspond to similar accuracies in less training time.
Other interesting hyperparameters are \textit{max\_seq\_length} and \textit{learning\_rate}.
Lowering the former to the expected maximum number of tokens in sentences reduces training and inference time.

Observe that joint training generalizes to sequential labeling and sentence classification.
In other words, any combination of tasks where sentences are classified as well as parts of the sentence and these tasks are correlated.
The tasks are expected to be correlated for any real-world NLP dataset.

\section{Implementation improvements}
\label{sec:implementation_improvements}
The current implementation is a proof-of-concept.
The loss function for joint training needs to be reconsidered.
Currently the importance of intent and all named-entities is the same, meaning one error in a intent or in a named-entity weights the same.
It might be better for the model to have a higher importance for the intents, since the intent contains information about the entire sentence.
Further tests are needed to see whether the increase in accuracy is significant.
This requires more datasets.
No dataset of around a few hundred distinct training examples has been evaluated, while this seems realistic for production use-cases.
One could merge AskUbuntu, WebApplications and Chatbot, but this would be an unrealistic dataset.
It would be a dataset where two thirds is related to software questions and one third is related to public transport information.
Tests should also be conducted using the other models such as \bert{multilingual} and \bert{base} (cased).
The authors~\citep{devlin2018} use English models to get SOTA results, this suggests that \bert{multilingual} performs worse.
It is unknown how big the difference between the two is.
Another improvement would be setting up fine-tuning such that TensorBoard visualisations (like figure~\ref{fig:tensorboard}) are possible.
This allows users to get a better insight into the model training.
Lastly, the code is in need of refactoring and validation.
For example, the NER code has a distinction between X and O.
O is used by the BIO annotation standard and X is used for positions which are not used (since the sentence is shorter than the maximum sequence length).
The model in its current implementation does use X to classify tokens.

% less important improvements
Less important improvements are using conditional code to fix I-\textsc{entity} statements appearing without a B-\textsc{entity}.
This constraint is enforced by the BIO annotation standard, where `intermediate' statements are preceded by `begin' statements.
Also, for production use it might be beneficial to do some hyperparameter tuning.

% todo: general suggestions such as validate training data (find outliers and duplicates)
% todo: check accuracy on datasets having more than intent and entity, but also part of speech tagging or whatever