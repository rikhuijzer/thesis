\chapter{\textsc{bench}}
\label{ch:bench}

% intro
The benchmarking tool is called \textsc{bench} and located on Github (\url{https://github.com/rikhuijzer/bench}).
Its goal is to be a reproducible benchmarking tool for intent classification and named-entity recognition.
Reproducible means that other people can run the code for their own use, or to reproduce results presented in this report.
The code is written in Python, since it is the default choice for machine learning.
Python is conceived as a object oriented language.
Over time it has included more and more functional programming ideas.
The code in this project will aim to be adhering to functional programming.
Reasons are pedagogic value, improved modularity, expressiveness, ease of testing, and brevity.
Some general notes on functional programming in Python are listed in Appendix~\ref{ch:fp}.

% final code remarks on fp and imports
The functional programming constraints for the project are that we do not define any new classes.
Specifically, we do not use the class keyword.
Exceptions being NamedTuples and Enums.
Both do provide functional APIs, but these are not fully supported by the autocomplete in PyCharm.
The code prefers returning iterators over collections, the reason for this is explained in Appendix~\ref{ch:demonstration}.
A final remark is about the imports.
When importing an attempt is made to explicitly import using `from $<$module$>$ import $<$class$>$`.
When more implicit imports are used `import $<$module$>$` this is can have multiple causes.
It is either caused by the appearance of circular imports, by the fact that some names are too common or to avoid reader confusion.
An example for the latter are the types defined in \textsc{src.typ}.
The names are quite generic and could cause name clashing or confusion when imported explicitly.

\section{Usage}
\label{sec:usage}
% set-up
Installation is similar to other Python projects.
Pull the code and in a terminal set the current working directory to the project folder.
Install the required pip packages by running the following command.
\begin{verbatim}
    pip install -r requirements.txt
\end{verbatim}
If one wants to check accuracy for an open-source system then run the following command.
\begin{verbatim}
    docker-compose up
\end{verbatim}
\textsc{docker-compose} will read `docker-compose.yml' and use that information to spin up various Docker containers.
All dockers listed in the file are available from Docker Hub.
This avoids having to build Dockers manually.
DeepPavlov has been removed from the configuration file, since it was found to be unstable, see Section~\ref{subsec:deeppavlov}.

% final parameters and run
After the set-up the program can be executed by running `bench.py'.
To change on which system the benchmarking occurs, replace the first parameter in the \textsc{get\_system\_corpus} call.
The prefix is used to determine which system is being tested.
Possible prefix options are `mock', `rasa', `deeppavlov', `lex' and `dialogflow'.
Rasa and DeepPavlov will use the complete string to find a matching port from `docker-compose.yml'.
So, based on the Docker configuration one can also specify `rasa-tensorflow', `rasa-spacy' or `rasa-mitie'.
The corpus (dataset) to run the bench on is specified by an enumerable, see \textsc{src.typ.Corpus} for possible options.
When running the script in a modern IDE autocomplete will suggest the possible corpora.
Slightly more convenient would be to have the script take input arguments using \textsc{sys.argv}.
After setting the two parameters the script can be executed and will display all predictions as well as micro, macro and weighted f1 scores.
The predictions and f1 scores will also be written to files, see the `results' folder.

\section{Overview}
\label{sec:overview}
A high-level code overview will be presented.
Since the code does not contain classes, the overview is simply a tree-like structure.
This is analogous with a book, where subsections are contained in sections and sections are contained in chapters.
In the code small functions are called by larger functions and these larger functions are called by even larger functions.
For an overview this idea can be generalized to modules.
An overview for the modules of \textsc{bench} is roughly as follows.
The `.py' suffix is omitted for all elements in the tree.
Tests are also omitted.

\begin{itemize}
    \item \textsc{bench}
    \begin{itemize}
        \item \textsc{src.utils}
        \item \textsc{src.typ}
        \item \textsc{src.dataset}
        \begin{itemize}
            \item \textsc{src.datasets.corpora}
            \item \textsc{src.datasets.snips}
        \end{itemize}
        \item \textsc{src.system}
        \begin{itemize}
            \item \textsc{src.systems.amazon\_lex}
            \item \textsc{src.systems.deeppavlov}
            \item \textsc{src.systems.dialogflow}
            \item \textsc{src.systems.mock}
            \item \textsc{src.systems.rasa}
            \item \textsc{src.systems.watson}
        \end{itemize}
        \item \textsc{src.evaluate}
        \begin{itemize}
            \item \textsc{src.results}
        \end{itemize}
    \end{itemize}
\end{itemize}

Some generic functions are listed in \textsc{src.utils} and used through the entire project.
The project makes use of type hints as introduced in Python 3 (and via comments in Python 2.7).
Also, the project does not use classes and therefore tends to pass more data through functions.
To define containers for these data NamedTuples are used.
A more in-depth explanation of why these are needed can be found in Section~\ref{sec:named_tuple}.
All NamedTuples or `types' are defined in \textsc{src.typ}.
The module also contains enumerable's or `Enums'.
These are used in cases where function behaviour depends on some parameter from a fixed set of options.
Alternatively one could use strings for these cases depending on user-preference.

The real work of the project is done by \textsc{src.dataset}, \textsc{src.system} and \textsc{src.evaluate}.
`Dataset' takes input files and converts them to an internal representation as defined by \textsc{src.typ}.
Input files here denote the original dataset files as created by the dataset publishers.
For the internal representation a Rasa Message is used, specifically \textsc{rasa\_nlu.training\_data.message.Message}.
The benefit of this is that it avoids defining the same structure and that it can be used in combination with Rasa code.
For example, \textsc{src.dataset.convert\_message\_to\_annotated\_str} uses Rasa code to print the internal data representation as a sentence in Markdown format (Section~\ref{subsec:format}).
Next, the data reaches \textsc{src.system}.
Here it is passed to the system under consideration, either in training or prediction mode.
For the predictions this is done by finding out which function can convert \textsc{src.typ.Query} to \textsc{src.typ.Response}.
When, for example, Rasa is under consideration the function \textsc{src.systems.rasa.get\_response} is called.
DeepPavlov would be handled by \textsc{src.systems.deeppavlov.get\_response}.
PyCharm is known to have the best type inference for Python.
The IDE is not yet able to infer function type for a function mapping, even when all functions have the same input and output type.
A workaround is to manually define the type of the function returned by the mapping as \verb|func: Callable[[tp.Query], tp.Response] =| $\cdots$.
\textsc{src.evaluate} takes all responses \textsc{tp.Response} evaluates the performance of the system under consideration.
Printing f1 score is a matter of three functions and about a dozen lines of code.
At one point more advanced logging has been included which is responsible for the other 12 functions and 110 lines of code.
